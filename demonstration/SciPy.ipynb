{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SciPy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `scipy` package contains several _sub-packages_ defining modules that provide efficient, **ready-to-use** implementations of numerous computational tools and algorithms that can be invoked independently or to incorporated into more complicated **customized** scientific analyses.\n",
    "\n",
    "The SciPy project provides [extensive online reference documentation](http://docs.scipy.org/doc/scipy/reference/index.html#reference) as well as a helpful [tutorial](http://docs.scipy.org/doc/scipy/reference/tutorial/index.html) that provides a pedagogical introduction to many of `scipy`'s most useful modules.\n",
    "\n",
    "The modules provided by `scipy` integrate transparently with the `numpy` and `matplotlib` packages, and leverage the powerful **numerical analysis** and **plotting** functionality they provide.\n",
    "\n",
    "To take advantage of this package integration, the `numpy` and `matplotlib` packages must be independently `import`ed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy\n",
    "import matplotlib\n",
    "from matplotlib import pyplot\n",
    "# enable inline display of plotted figures in the IPython Notebook interface\n",
    "%matplotlib inline "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `scipy` package must also be `import`ed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import scipy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see the list of sub-packages that `scipy` provides, use the `help(...)` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "help(scipy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sub-packages of `scipy` provide extensive and diverse functionality. In subsequent sections, use of `scipy` will be demonstrated using a very restricted but (hopefully) illustrative subset of the package contents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Physical and Mathematical Constants and Units"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scientific computation often involves evaluation of formulae that are defined in terms of mathematical constants like $\\pi$ and empirically determined physical values that are also **nominally** constant. The [`scipy.constants`](http://docs.scipy.org/doc/scipy/reference/constants.html) package defines a useful repository that provides **high precision** numeric values for many physical and mathematical constants to be used for scientific computation.\n",
    "\n",
    "The `scipy.constants` subpackage includes numeric values for the scaling factors that correspond to standard **unit prefixes** e.g. nano, milli, kilo etc.\n",
    "\n",
    "The package also provides an interface to the table of [CODATA Recommended Values of the Fundamental Physical Constants 2010](http://physics.nist.gov/cuu/Constants/index.html) defined by the _National Institute of Science and Technology_ (NIST). This interface to the CODATA table also also defines the **units** and **relative measurement uncertainties** for each of the provided constants.\n",
    "\n",
    "Finally, the `scipy.constants` package defines several functions that perform **unit conversion operations**.\n",
    "\n",
    "#### Step 1: Import `scipy.constants`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy import constants"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A list of the numeric values that are provided by the `scipy.constants` package is available [online](https://docs.scipy.org/doc/scipy-0.15.1/reference/constants.html) or **inline** using the `help(...)` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "help (scipy.constants)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2: Excercise the Standard Interface to `scipy.constants`\n",
    "\n",
    "The *standard* (non-CODATA) interface that is defined by `scipy.constants` provides the values of physical and mathematical constants as variables that are defined as symbols within the package namespace. Accordingly, their values can be reference dusing the \"`.`\" operator. \n",
    "\n",
    "Consider a concrete example involving the mathematical constants $\\pi$ and $\\phi$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print \"The value of \" + u\"\\u03c0\" + \" is\", scipy.constants.pi\n",
    "print \"The value of \" + u\"\\u03A6\" + \" is\", scipy.constants.golden_ratio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Physical** constants that are provided via the standard `scipy.constants` interface are defined in terms of **SI units**. Consider another concrete example involving the elementary charge $e$ and the electron mass $m_{e}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print \"The value of the elementary charge is\", scipy.constants.elementary_charge, \"Coulombs\"\n",
    "print \"The value of the electron mass is\", scipy.constants.electron_mass, \"kg\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The standard `scipy.constants` interface also provides the values of various non-SI units in terms of SI units. The subsequent example prints the kilogram values that correspond to one ounce, one pound and one stone.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print \"1 ounce =>\", scipy.constants.ounce, \"kg\"\n",
    "print \"1 pound =>\", scipy.constants.pound, \"kg\"\n",
    "print \"1 stone =>\", scipy.constants.stone, \"kg\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3: Excercise the CODATA interface to `scipy.constants`\n",
    "\n",
    "The identifier `scipy.constants` package stores the CODATA table in a _Python_ `dict` with the identifier `physical_constants`. The `dict` that is referenced by `scipy.constants.physical_constants` maps `str`-type keys to three-element `tuple`-type values. The keys correpond to the **name** of the tabulated constant, while the values store the **associated numeric data**. The elements of each `tuple` define\n",
    "\n",
    "1. The numeric value of the constant, stored as a `float` type.\n",
    "2. The associated unit, stored as a `str`-type.\n",
    "3. The associated measurement uncertainty, stored as a `float` type.\n",
    "\n",
    "The subsequent example extracts the tabulated data for the the value of $c$, which corresponds to the `dict` key \"`speed of light in vacuum`\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print scipy.constants.physical_constants['speed of light in vacuum']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Individual data corresponding to a particular **`key`** can be extracted using functions that are defined by the `scipy.constants` package.\n",
    "\n",
    "1. The `scipy.constants.value(`**`key`**`)` function extracts and returns the numeric **value** of the constant corresponding to the specified **`key`**.\n",
    "2. The `scipy.constants.unit(`**`key`**`)` function extracts and returns the `str`-type **unit** of the constant corresponding to the specified **`key`**.\n",
    "3. The `scipy.constants.precision(`**`key`**`)` function extracts and returns the numeric value of the **relative measurement uncertainty** associated with constant value corresponding to the specified **`key`**.\n",
    "\n",
    "**Note:** If an invalid key is supplied to any of these functions, a `KeyError` exception is raised."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print \"Value of the alpha particle mass is\", scipy.constants.value(\"alpha particle mass\")\n",
    "print \"Associated units of the alpha particle mass are\", scipy.constants.unit(\"alpha particle mass\")\n",
    "print \"Associated relative measurement uncertainty for alpha particle mass is\",scipy.constants.precision(\"alpha particle mass\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `scipy.constants.find(...)` function searches the `scipy.constants.physical_constants` `dict` for a **`key`** that matches all or part of an optional `str`-type keyword argument with the identifier `sub`. If the `sub` argument is omitted or empty, then all keys in the `scipy.constants.physical_constants` `dict` are returned. A second optional keyword argument `disp` accepts a boolean value. If the supplied value is `False`, then the matching keys are returned as the elements of a _Python_ `list`. Otherwise, the matching keys are simple printed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Pick a constant to search for!\n",
    "print scipy.constants.find(sub=\"\", disp=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 4: Excercising the Unit Conversion Functions Provided by `scipy.constants`\n",
    "\n",
    "For some unit conversions that require more than simple multiplicative scaling (e.g. the conversion between _Celsius_ and _Fahrenheit_), the `scipy.constants` package provides **functions** that implement the conversion operation.\n",
    "\n",
    "Six functions are provided to perform temperature unit conversions between degrees of _Celsius_, _Fahrenheit_, and _Kelvin_.\n",
    "\n",
    "1. `scipy.constants.C2K(`**`tempInCelsius`**`)` - Converts its argument from _Celsius_ to _Kelvin_.\n",
    "2. `scipy.constants.K2C(`**`tempInKelvin`**`)` - Converts its argument from _Kelvin_ to _Celsius_.\n",
    "3. `scipy.constants.F2C(`**`tempInFahrenheit`**`)` - Converts its argument from _Fahrenheit_ to _Celsius_.\n",
    "4. `scipy.constants.C2F(`**`tempInCelsius`**`)` - Converts its argument from _Celsius_ to _Fahrenheit_.\n",
    "5. `scipy.constants.F2K(`**`tempInFahrenheit`**`)` - Converts its argument from _Fahrenheit_ to _Kelvin_.\n",
    "6. `scipy.constants.K2F(`**`tempInKelvin`**`)` - Converts its argument from _Kelvin_ to _Fahrenheit_.\n",
    "\n",
    "Two additional functions are provided to perform the reciprocal mapping between the _frequency_ ($\\nu$) and _wavelength_ ($\\lambda$) of electromagnetic radiation. \n",
    "\n",
    "1. `scipy.constants.lambda2nu(`**`wavelength`**`)` - Maps its argument from _wavelength_ to _frequency_ space.\n",
    "2. `scipy.constants.nu2lambda(`**`frequency`**`)` - Maps its argument from _frequency_ to _wavelength_ space.\n",
    "\n",
    "The mapping is defined in terms of and the speed of light in vacuum $c$ according to $\\lambda=c/\\nu$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Define a temperature in degrees Fahrenheit\n",
    "coldInFahrenheit = 20.0\n",
    "# Convert to the equivalent value in degrees Celcius\n",
    "coldInCelsius = scipy.constants.F2C(coldInFahrenheit)\n",
    "print coldInFahrenheit, \"degrees Fahrenheit is equivalent to\", coldInCelsius, \"degrees Celsius\"\n",
    "\n",
    "# Define a wavelenth in nanometres.\n",
    "wavelengthInNm = 700.0\n",
    "\n",
    "'''\n",
    "Map the wavelength value into frequency space.\n",
    "\n",
    "Note that the scipy.constants package also defines scalings that correspond\n",
    "to vaious unit prefixes e.g. nano, milli etc \n",
    "'''\n",
    "# Convert to the equivalent value in degrees Celcius\n",
    "frequencyInHz = scipy.constants.lambda2nu(wavelengthInNm*scipy.constants.nano)\n",
    "print \"A wavelength of\",wavelengthInNm, \"nm corresponds to a frequency of\", frequencyInHz, \"Hz\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Signal Processing - The Fast Fourier Transform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **Fast Fourier Transform** (FFT) algorithm is a particulary efficient method for computing the [**Discrete Fourier Transform**](http://en.wikipedia.org/wiki/Discrete_Fourier_transform) (DFT) and is a cornerstone of numerous computational **signal processing** analyses. Given a discretely sampled sequence $x[n]$ of $N$ complex-valued data, the DFT $y[k]$ is a complex-valued sequence defined coording to \n",
    "\n",
    "$$y[k] = \\sum_{n=0}^{N-1} e^{-2 \\pi j \\frac{k n}{N} } x[n] \\, ,$$\n",
    "\n",
    "The DFT is invertable. The inverse transformation is defined according to\n",
    "\n",
    "$$x[n] = \\frac{1}{N} \\sum_{n=0}^{N-1} e^{2 \\pi j \\frac{k n}{N} } y[k] \\, .$$\n",
    "\n",
    "The `scipy.fftpack` subpackage implements a flexible and intuitive interface that facilitates application of the FFT and its inverse for multidimesnional datasets. The following example demonstrates application of a simple one-dimensional FFT and its inverse."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1: Import `scipy.fftpack` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "''' \n",
    "Begin by importing the fftpack subpackage, allowing its symbols to be referenced\n",
    "with appropriate qualification.\n",
    "'''\n",
    "from scipy import fftpack"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The functionality provided by `fftpack` is documented [online](http://docs.scipy.org/doc/scipy/reference/fftpack.html#module-scipy.fftpack) and can be accessed *inline* using the `help(...)` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "help(scipy.fftpack)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `scipy.fftpack.fft` function implements a simple one-dimensional FFT. Use the `help(...)` function again to determine what arguments it requires."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "help(scipy.fftpack.fft)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2: Generate an Artifical Dataset to Transform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The FFT operates on sequence of values that are assumed to form a discretely sampled approximation of a continuous function. The `numpy` package can be used to generate a simple artificial dataset that samples a continuous function and injects some random noise to model instrumental uncertainties.\n",
    "\n",
    "Begin by defining the sampling range and the number of function samples to generate. These values will be reappear several times, so it is sensible to define corresponding `sampleRange` and `numSamples` variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Define a tuple-type variable corresponding to the number of range of values to sample\n",
    "sampleRange = (0, 2*numpy.pi)\n",
    "\n",
    "# Define a float-type variable corresponding to the number of function samples\n",
    "numSamples = 100."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate `numSamples` $x$ equidistant values between 0 and $2\\pi$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "xValues = numpy.arange(sampleRange[0], sampleRange[1], sampleRange[1]/numSamples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate noise-free samples of $y=\\sin(2x) + 0.5\\sin(5x)$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "yValuesClean = numpy.sin(2*xValues) + 0.5*numpy.sin(5*xValues)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate a second dataset with uniformly distributed noise to the sample values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "yValuesDirty = yValuesClean + numpy.random.uniform(-0.25, 0.25, numSamples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3: Plot the Artificial Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Adjust the size of the current figure\n",
    "pyplot.figure(figsize=(10, 8))\n",
    "# Plot the original and noisy signals on the current axes \n",
    "plottedCurves = pyplot.plot(xValues, yValuesClean, xValues, yValuesDirty)\n",
    "\n",
    "'''\n",
    "Add a legend. Note the use of the loc and ncol keyword arguments to reposition \n",
    "and reshape the legend.\n",
    "'''\n",
    "pyplot.legend((plottedCurves[0], plottedCurves[1]), ('Original Signal', '\"Noisy\" Signal'), loc='lower center', ncol=2)\n",
    "\n",
    "# Customize plot axes\n",
    "pyplot.ylim(-2.5, 2)\n",
    "'''\n",
    "Note that since pyplot.xlabel(...) and pyplot.ylabel(...) return matplotlib.text.Text \n",
    "instances, the set_fontsize(...) method can be invoked directly.\n",
    "'''\n",
    "pyplot.xlabel(r\"$x$\").set_fontsize(15)\n",
    "pyplot.ylabel(r\"$y$\").set_fontsize(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 4: Perform the forward FFT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Invoke the `scipy.fftpack.fft(...)` function to perform the one-dimensional FFT. The simplest invocation requires a single array-type argument containing the $y$-value samples to which the FFT should be applied.\n",
    "\n",
    "The function returns a `numpy.array` with `numElements` complex-valued elements corresponding to the FFT result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fftVals = scipy.fftpack.fft(yValuesDirty)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output of the `help(...)` function for the `scipy.fftpack.fft` function details the the appropriate interpretation of the elements of the `array` that is returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "help (scipy.fftpack.fft)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In summary:\n",
    "\n",
    "1. The zeroth element of `fftVals` contains the zero-frequency term.\n",
    "2. Elements between 1 and $N/2$ contain the positive-frequency terms, in order of increasing frequency.\n",
    "3. The remaining $N/2-1$ elements contain the negative-frequency terms, in order of decreasing frequency.\n",
    "\n",
    "**Note** that the `scipy.fftpack.fftshift(...)` function rearranges the elements of the `numpy.array` that is returned by `scipy.fftpack.fft`, such that the elements are in strictly increasing order of frequency from the most negative to the most positive with the zero-frequency term at the midpoint.\n",
    "\n",
    "**Note** that the `scipy.fftpack.fftfreq(...)` function enables computation the frequency **values** that correspond to each of the elements of the `numpy.array` that is returned by `scipy.fftpack.fft`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Generate a numpy.array that simply enumerates the elements of the \n",
    "numpy.array that is returned by scipy.fftpack.fft.\n",
    "'''\n",
    "fftElements = numpy.arange(0,numSamples,1)\n",
    "\n",
    "'''\n",
    "Generate a numpy.array that defines the frequencies that correspond to\n",
    "the elements of the numpy.array that is returned by scipy.fftpack.fft.\n",
    "'''\n",
    "sampleSpacing = (sampleRange[1] - sampleRange[0])/numSamples\n",
    "fftFrequencies = scipy.fftpack.fftfreq(int(numSamples), sampleSpacing)\n",
    "\n",
    "'''\n",
    "Generate a numpy.array that defines the frequencies that correspond to\n",
    "the elements of the numpy.array that is returned by scipy.fftpack.fft,\n",
    "rearranged into strictly increasing order.\n",
    "'''\n",
    "\n",
    "shiftedFftFrequencies = scipy.fftpack.fftshift(fftFrequencies)\n",
    "\n",
    "'''\n",
    "Apply a similar shift to the FFT components comprising the numpy.array \n",
    "that is returned by scipy.fftpack.fft.\n",
    "'''\n",
    "shiftedFftVals = scipy.fftpack.fftshift(fftVals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **magnitude** of each complex-valued element encodes the contribution of sinusoidal components with the corresponding frequency to the sampled signal. The `numpy.abs(...)` function computes the magnitude of all elements in the FFT result array using a single operation.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "contributions = numpy.abs(fftVals)\n",
    "shiftedContributions = numpy.abs(shiftedFftVals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 5: Plot the Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the computed contributions of each frequency to the input signal as a bar chart, using the `pyplot.bar(...)` function. Generate two plots:\n",
    "\n",
    "1. A plot illustrating the computed magnitude of **all** elements of `fftVals`, in the order they are returned.\n",
    "2. A plot illustrating the first $N/2$ elements of `fftVals` corresponding to the zero and positive-frequency components.\n",
    "3. A plot illustrating a _thresholded_ copy of the computed magnitude of **all** elements of `fftVals` with any values that are less than 10 set equal to zero. **Note** the use of a `numpy.array` of boolean values to provide an _indexing mask_ that selects array elements that fulfil a particular criterion.\n",
    "\n",
    "Note that for purely real input data, the **magnitudes** of the corresponding positive and negative frequency components are identical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Customize the size of the current figure.\n",
    "pyplot.figure(figsize=(15, 4))\n",
    "\n",
    "'''\n",
    "Define a grid of two horizontally arranged axis pairs. \n",
    "\n",
    "Plot the computed magnitude of all elements of \"fftVals\".\n",
    "'''\n",
    "pyplot.subplot(1,3,1)\n",
    "pyplot.bar(fftElements, contributions, color='g', width=1)\n",
    "pyplot.xlabel(\"Frequency Component\").set_fontsize(15)\n",
    "pyplot.ylabel(\"Signal Contribution\").set_fontsize(15)\n",
    "\n",
    "'''\n",
    "Plot the first N/2 elements of \"fftVals\" corresponding to the zero \n",
    "and positive-frequency components.\n",
    "'''\n",
    "pyplot.subplot(1,3,2)\n",
    "pyplot.bar(shiftedFftFrequencies, shiftedContributions, color='r', width=2*sampleSpacing)\n",
    "pyplot.xlabel(\"Frequency\").set_fontsize(15)\n",
    "pyplot.ylabel(\"Signal Contribution\").set_fontsize(15)\n",
    "\n",
    "'''\n",
    "Plot a thresholded copy of the computed magnitude of all elements\n",
    "of fftVals with any values that are less than 10 set equal to zero.\n",
    "'''\n",
    "pyplot.subplot(1,3,3)\n",
    "\n",
    "'''\n",
    "Generate a numpy.array containing a boolean-value that corresponds to each\n",
    "element of the \"contributions\" array. The elements of this array should be \n",
    "equal to \"True\" if the the value of the corresponding element of \"contributions\"\n",
    "is < 10 and \"False\" otherwise.\n",
    "'''\n",
    "lowContributionIndices = shiftedContributions < 10\n",
    "# Make a deep copy of the shiftedContributions array\n",
    "highContributions = shiftedContributions.copy()\n",
    "'''\n",
    "Use the \"lowContributionIndices\" as an indexing mask to set only those elements\n",
    "of \"highContributions\" that correspond to True-valued elements of \n",
    "\"lowContributionIndices\" to zero.\n",
    "'''\n",
    "highContributions[lowContributionIndices] = 0\n",
    "pyplot.bar(shiftedFftFrequencies, highContributions, color='b',  width=2*sampleSpacing)\n",
    "pyplot.xlim(-8, 8)\n",
    "pyplot.xlabel(\"Frequency\").set_fontsize(15)\n",
    "pyplot.ylabel(\"Signal Contribution\").set_fontsize(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 6: Perform an Inverse FFT of a **Filtered** Contribution Spectrum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the `lowContributionIndices` array to generate a filtered copy of the complex-valued `fftVals` array that was returned by the `scipy.fftpack.fft(...)` function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Make a deep copy of the numpy.array of complex-valued elements that\n",
    "was returned by scipy.fftpack.fft(...)\n",
    "'''\n",
    "filteredFftVals = fftVals.copy()\n",
    "\n",
    "'''\n",
    "Generate an indexing mask that retains only the zero-frequency component\n",
    "and the components with magnitudes greater than 10.\n",
    "'''\n",
    "filteredIndices = numpy.abs(filteredFftVals) < 10\n",
    "filteredIndices[0] = False\n",
    "\n",
    "'''\n",
    "Set all unmasked elements of the deep copy array to zero.\n",
    "'''\n",
    "filteredFftVals[filteredIndices] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now invoke the `scipy.fftpack.ifft(...)` function to perform the inverse FFT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Perform an inverse FFT of the filtered complex-valued array\n",
    "filteredYVals = scipy.fftpack.ifft(filteredFftVals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 7: Plot the Result of the Inverse FFT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Adjust the size of the current figure\n",
    "pyplot.figure(figsize=(10, 8))\n",
    "\n",
    "''' Plot three curves corresponding to:\n",
    "1. The result of the FFT filtering process.\n",
    "2. The noise-free function samples.\n",
    "3. The noisy function samples.\n",
    "'''\n",
    "plottedCurves = pyplot.plot(xValues, numpy.real(filteredYVals), xValues, yValuesClean, xValues, yValuesDirty)\n",
    "'''\n",
    "Add a plot legend and adjust the y-limits of the current axes\n",
    "to accommodate its extent.\n",
    "'''\n",
    "pyplot.legend((plottedCurves[0], plottedCurves[1], plottedCurves[2]), (\"Filtered\",\"Clean\",\"Noisy\"), loc='lower center', ncol=3)\n",
    "newLimits = pyplot.ylim(-2.5, 2.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simulating Artificial Datasets, Fitting and Optimization\n",
    "\n",
    "Many scientific analyses involve modeling experimental data using phenomenological or physically motivated analytical functions. The `scipy` package provides several subpackages that enable straightforward construction of parameterized analytical models. See the online tutorials on [basic](http://docs.scipy.org/doc/scipy/reference/tutorial/basic.html) and [special](http://docs.scipy.org/doc/scipy/reference/tutorial/special.html) parametric functions for more details.\n",
    "\n",
    "If experimental data are not straightforwardly modeled using simple analytical functions, **Monte Carlo simulations** are often used in modern scientific analyses to provide artificial datasets that can be compared with measurement results. Such simulations often require the generation pseudo-randomly distributed datasets that conform to a particular expected **distribution function**.\n",
    "\n",
    "The `scipy` package includes the `stats` subpackage, which in turn contains several \"_Random Variable_\" classes that encapsulate the properties of **over 90** of the most commonly ecountered distribution functions.\n",
    "\n",
    "The \"_Random Variable_\" classes provide a number of methods that compute statistical properties of the distributions they describe such as the _Probablility Density Function_ (PDF) and the _Cumulative Density Function_ (CDF). They also provide methods to generate point **samples of values** that conform to the modeled distribution.  \n",
    "\n",
    "The [`scipy.stats`](http://docs.scipy.org/doc/scipy/reference/stats.html#module-scipy.stats) package provides [extensive online documentation](http://docs.scipy.org/doc/scipy/reference/stats.html), and an [introductory tutorial](http://docs.scipy.org/doc/scipy/reference/tutorial/stats.html) is also available. \n",
    "\n",
    "Assuming a particular data-modelling strategy, the [`scipy.optimize`](http://docs.scipy.org/doc/scipy/reference/optimize.html#module-scipy.optimize) package defines several functions that can be used to _fit_ models to experimentally measured data.\n",
    "\n",
    "#### Step 1: Import the `scipy.stats` subpackage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Import the scipy.stats subpackage, which provides facilities to\n",
    "simulate artificial datasets.\n",
    "'''\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2: Generate an Artificial Dataset.\n",
    "All of the \"_Random Variable_\" classes that are provided by `scipy.stats` package provide an `rvs(...)` method that returns a `numpy.array` containing a user-specified number of numeric elements with values that are randomly generated in accordance with the distribution function that the \"_Random Variable_\" class models.\n",
    "\n",
    "The number of arguments that are **required** by each `rvs(...)` method is equal to the number of **parameters** that define the distribution function that the associated \"_Random Variable_\" class models. The `scipy` documentation refers to these required arguments as **shape parameters**.\n",
    "\n",
    "The optional **keyword argument** `size` controls the number of data that are generated by the `rvs(...)` method. If `size` is not provided, a single randomly generated value is returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "help(scipy.stats.poisson.rvs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `scipy.stats.poisson` class models the [_Poisson_](http://en.wikipedia.org/wiki/Poisson_distribution) distribution function. \n",
    "$$P(x|\\lambda)=\\frac{{e^{ - \\mu } \\mu^x }}{{x!}}$$\n",
    "The _Poisson_ distribution is a discrete probability distribution that expresses the probability of $x$ events occurring in a fixed interval of time and/or space if these events occur with a known average rate $\\mu$ and independently of the time since the last event. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "help(scipy.stats.poisson)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The _Poisson_ distribution is completely characterized by the **single shape parameter** `mu`. The following invocation of `scipy.stats.poisson.rvs(...)` generates a `numpy.array` containing 2000 randomly generated $x$-values. The generated values could be interpreted as independent **measurements** of the **number of events per interval** that are consistent with a _Poisson_ process for which the **expected rate $\\mu=5$ events per interval**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Generate 2000 random variates corresponding to measured event counts\n",
    "per interval that are consistent with a Poisson process for which the \n",
    "expected rate \"mu\" is 5 events per interval.\n",
    "'''\n",
    "mu = 5\n",
    "poissonValues = scipy.stats.poisson.rvs(mu, size=2000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3: Plot a Histogram of the Simulated Data\n",
    "**Histograms** are typically used to visualize the distributions of unbinned datasets. The `matplotlib.pyplot` module provides the `hist(...)` function to generate and render histogram of unbinned datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "help(matplotlib.pyplot.hist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `matplotlib.pyplot.hist(...)` function **requires** a single argument which should be a _Python_ sequence or `numpy.array` containing unbinned values to be histogrammed. The histogram generation and rendering can be customized using several **keyword arguments**.\n",
    "\n",
    "The invocation of `matplotlib.pyplot.hist(...)` in the subsequent cell uses the `bins` keyword argument to generate a histogram with 15 bins instead of the default 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Adjust the size of the current figure\n",
    "pyplot.figure(figsize=(10, 8))\n",
    "\n",
    "'''\n",
    "Generate and plot a histogram of the Poisson-distributed values that were\n",
    "generated in Step 2. The hist(...) function returns data describing the \n",
    "bins of the generated histogram and thier contents.\n",
    "'''\n",
    "binProperties = pyplot.hist(poissonValues, bins=15)\n",
    "\n",
    "# Customize the labels of the current axes\n",
    "pyplot.xlabel(r'$x$').set_fontsize(20)\n",
    "pyplot.ylabel('Frequency').set_fontsize(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 4: Fit a Function to Unbinned Data\n",
    "The `scipy.optimize` package provides the generic `minimize(...)` function, which determines the set of **parameter values $\\mathbf{\\Theta} = \\{\\theta_{1}, \\theta_{2}, \\ldots, \\theta_{n}\\}$** that **minimizes** the value of a specified **but arbitrary** function $f(\\mathbf{x})$ (of the dataset $\\mathbf{x}=\\{x_{1}, x_{2}, \\ldots, x_{m}\\}$) that they parameterize.\n",
    "\n",
    "By providing an **appropriate definition** for $f(\\mathbf{x})$, the `scipy.optimize.minimize(...)` function can be uset to **fit** a parametric model function to an experimental dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Import the scipy.optimize subpackage\n",
    "from scipy import optimize\n",
    "\n",
    "help(scipy.optimize.minimize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The log-likelihood $\\mathcal{L}(\\mathbf{\\Theta} | \\mathbf{x})$ is commonly minimized when fitting a probabilistic model to data. This function computes the logarithm of the probability that a model described by a particular set of  parameters would result in the observed dataset. \n",
    "\n",
    "Consider the concrete example of the _Poisson_ distribution, for which $\\mathbf{\\Theta}\\rightarrow\\mu$.\n",
    "\n",
    "$$\\mathcal{L}\\left(\\mu|\\mathbf{x}\\right) = \\displaystyle\\sum_{i=1}^{m}\\ln P\\left(x_{i}|\\mu\\right) = \\displaystyle\\sum_{i=1}^{m}\\ln\\left(\\frac{{e^{ - \\mu } \\mu^{x_{i}} }}{{x_{i}!}}\\right)$$\n",
    "\n",
    "Since $P\\left(x_{i}|\\mu\\right)$ is a probability and is necessarily positive, $-\\mathcal{L}\\left(\\mu|\\mathbf{x}\\right)$ should be provided as the `fun` argument to `scipy.optimize.minimize(...)`.\n",
    "\n",
    "The _Random Variable_ classes that are included in the `scipy.stats` package provide methods that compute the **probability** $P\\left(\\mathbf{x}^{*}|\\mathbf{\\Theta}^{*}\\right)$ of obtaining a **particular** measurement $\\mathbf{x}^{*}$, given a **particular** parameter set $\\mathbf{\\Theta}^{*}$. For **discrete** probability distributions the `pmf(...)` (_Probability Mass Function_) method performs the required computation. For **continuous** probability distributions, the `pdf(...)` (_Probability Density Function_) method should be invoked. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def poissonLikelihood(mu, data) :\n",
    "    '''\n",
    "    A function that computes the negative log-likelihood that \n",
    "    a Poisson process with expected rate \"mu\" resulted in generation\n",
    "    of the observed dataset \"data\".\n",
    "    \n",
    "    ARGUMENTS:\n",
    "    ==========\n",
    "    mu - The mu argument should be a float-type specifying the expected\n",
    "    rate of the modeled Poisson process.\n",
    "    data - The data argument should be a numpy.array containing experimental\n",
    "    data\n",
    "    \n",
    "    '''    \n",
    "    '''\n",
    "    Invoke the scipy.stats.poisson.pmf(...) method to compute the \n",
    "    probability of that element of \"data\" would be produced by a Poisson\n",
    "    process with expected rate \"mu\".\n",
    "    \n",
    "    Note that the scipy.stats.poisson.pmf(...) method operates on the entire\n",
    "    data array and returns a numpy.array of the corresponding probabilities.\n",
    "    '''\n",
    "    dataLikelihoods = scipy.stats.poisson.pmf(data, mu)\n",
    "    \n",
    "    '''\n",
    "    Attempt to avoid problems associated with computing the logarithm of zero by \n",
    "    returning a very large positive result if any elements of \"dataLikelihoods\"\n",
    "    are zero. The numpy.count_nonzero(...) function is used.\n",
    "    '''\n",
    "    if numpy.count_nonzero(dataLikelihoods) < len(dataLikelihoods) :\n",
    "        return 1e20\n",
    "    \n",
    "    '''\n",
    "    Use the numpy.log(...) and numpy.sum(...) functions to compute the return\n",
    "    value.\n",
    "    '''\n",
    "    return -numpy.sum(numpy.log(dataLikelihoods))\n",
    "\n",
    "'''\n",
    "Sanity-test the \"poissonLikelihood(...)\" function for several test values of \"mu\"\n",
    "to verify that the minimum value corresponds to  \n",
    "'''\n",
    "print \"Assuming mu - 1 => \", poissonLikelihood(mu - 1, poissonValues)\n",
    "print \"Assuming mu => \", poissonLikelihood(mu, poissonValues)\n",
    "print \"Assuming mu + 1 => \", poissonLikelihood(mu + 1, poissonValues)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The subsequent cell contains an invocation of the `scipy.optimize.minimize(...)` function that will determine the value of $\\hat\\mu$ that is most likely to have resulted in the `poissonValues` dataset (**Recall** that `poissonValues` was generated using a true value of $\\mu=5$). The following values are supplied as function arguments:\n",
    "\n",
    "1. `fun`: The **identifier** of the `poissonLikelihood` function **without parentheses**. This is the generic mechanism by which function-types can be passed as function or method arguments in _Python_.\n",
    "2. `x0`: An _initial guess_ for the value of $\\hat\\mu$. A value of 1 is used to demonstrate that the `minimize` function does indeed converge towards $\\mu=5$.\n",
    "3. Keyword argument `args`: The generated `poissonValues` dataset is provided as a **single element** `tuple` using optional keyword argument `args`. In general, the elements of the `tuple` that is provided as the value of `args` will be unpacked and provided as separate arguments to the objective function that is supplied as the `fun` argument. \n",
    "\n",
    "The `scipy.optimize.minimize(...)` function returns an instance of the `scipy.optimize.OptimizeResult` class which defines a `numpy.array`-type member datum `x` that generally contains the values of the parameters ($\\mathbf{\\Theta}$) that minimize the value returned by the objective function supplied as the `fun` argument to `scipy.optimize.minimize(...)`. \n",
    "\n",
    "In this case, `x` contains a single element corresponding to the value of $\\hat\\mu$ that is most likely to have resulted in the generation of `poissonValues`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Use a value of 1 as an initial guess for the expected rate\n",
    "muGuess = 1\n",
    "\n",
    "'''\n",
    "Invoke the scipy.optimize.minimize(...) function to determine the \n",
    "value of mu that is most likely to have resulted in the \"poissonValues\" dataset. \n",
    "'''\n",
    "result = scipy.optimize.minimize(poissonLikelihood, muGuess, args=(poissonValues,))\n",
    "    \n",
    "'''\n",
    "Print the value of mu that minimizes the value returned by \"poissonLikelihood(...)\"\n",
    "Ideally this should be close to the value of mu=5 which was used to generate\n",
    "the \"poissonValues\" dataset.\n",
    "'''\n",
    "print \"Most likely Poisson rate is\", result.x[0], \"which differs from true rate by\", result.x[0] - mu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 5: Fit a Function to Binned Data\n",
    "The `scipy.optimize` package provides the `curve_fit(...)` function to enable straightforward fitting of specified **but arbitrary** functions to **binned** datasets using [nonlinear least-squares](http://en.wikipedia.org/wiki/Non-linear_least_squares) minimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "help(scipy.optimize.curve_fit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A _Binned Dataset_ can be nominally defined as **two** numeric sequences. The first sequence defines a series of coordinate values (nominally bin centres), while the second sequence defines arbitrary measured values (nominally bin contents) that correspond to the coordinate values defined by the elements of the first sequence.\n",
    "\n",
    "The subsequent cells demonstrate the generation of an artificial **binned** dataset by adding random Gaussian offsets to 50 equally spaced samples of an analytic one-dimensional function.\n",
    "$$f(x|\\Theta) = \\theta_{1}\\sin(\\theta_{2}x)+\\theta_{3}\\cos(\\theta_{4}x)$$\n",
    "where \n",
    "$$\\Theta = \\{\\theta_{1},\\theta_{2},\\theta_{3},\\theta_{4}\\}$$\n",
    "are the \"_true_\" function parameters.\n",
    "\n",
    "The `rvs(...)` method that is provided by the [`scipy.stats.norm`](http://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.norm.html) class is used to generate the required random Gaussian offsets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Generate a \"binned dataset\" using 50 function samples\n",
    "numSamples = 50\n",
    "'''\n",
    "Define a numpy.array that specifies a set of \"true\" parameter values\n",
    "to be used when generating an arificial \"binned dataset\".\n",
    "\n",
    "Use a constructor that accepts a Python list type\n",
    "and initializes its elements to match those of the provided list.\n",
    "'''\n",
    "trueParameters = numpy.array([0.3, 2, 0.8, 0.5])\n",
    "\n",
    "'''\n",
    "Use the numpy.linspace(...) function to generate a numpy.array containing\n",
    "50 equally spaced x-values between -5 and 5.\n",
    "'''\n",
    "xValues = numpy.linspace(-5, 5, numSamples)\n",
    "\n",
    "'''\n",
    "Generate an array of 50 \"clean\" values of f(x) evaluated for each of the \n",
    "elements of xValues, using the true function parameters.\n",
    "\n",
    "The numpy.sin(...) and numpy.cos(...) functions are used to operate on \n",
    "entire arrays at once.\n",
    "'''\n",
    "yValuesClean = trueParameters[0]*numpy.sin(trueParameters[1]*xValues)\n",
    "yValuesClean += trueParameters[2]*numpy.cos(trueParameters[3]*xValues)\n",
    "\n",
    "'''\n",
    "Use the rvs(...) method that is provided by the `scipy.stats.norm` class\n",
    "to generate a numpy.array of 50 randomly generated Gaussian distributed\n",
    "random offsets assuming a Gaussian mean of zero and a variance of 0.15\n",
    "'''\n",
    "yOffsets = scipy.stats.norm.rvs(0, 0.15, size=numSamples)\n",
    "\n",
    "'''\n",
    "Add the random offsets to the corresponding elements of the \"clean\" \n",
    "function samples.\n",
    "'''\n",
    "yValuesDirty = yValuesClean + yOffsets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first argument (`f`) that is required by `scipy.optimize.curve_fit(...)` method **must** be the identifier of a  function that accepts an $x$-coordinate value as its first argument and the **individual elements** of the parameter vector $\\mathbf{\\Theta}$ as its remaining arguments.\n",
    "\n",
    "The subsequent cell defines an appropriate function called `modelFunction` that can be fitted to the artificial (`xValues`, `yValuesDirty`) binned dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Define a function to be supplied to scipy.optimize.curve_fit(...) as the \n",
    "objective function argument \"f\".\n",
    "'''\n",
    "def modelFunction(x, p1, p2, p3, p4) :\n",
    "    '''\n",
    "    Function evaluates f(x) = p1*sin(p2*x) + p3*cos(p4*x) using the supplied\n",
    "    arguments. \n",
    "    \n",
    "    Use of numpy.sin(...) and numpy.cos(...) functions allows x and the \n",
    "    remaining arguments to be numpy array types so modelFunction is \n",
    "    implicitly a vector function.\n",
    "    '''\n",
    "    return p1*numpy.sin(p2*x) + p3*numpy.cos(p4*x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The subsequent cell contains an invocation of the `scipy.optimize.curve_fit(...)` function that will determine the values of the elements of $\\hat{\\mathbf{\\Theta}}$ that define the function $f(x|\\hat{\\mathbf{\\Theta}})$ that most closely approaches the values of `yValuesDirty` at each of the corresponding $x$-coordinates that are specified by `xValues`. The following values are supplied as function arguments:\n",
    "\n",
    "1. `f`: The **identifier** of the `modelFunction` function **without parentheses**.\n",
    "2. `xdata`: A `numpy.array` specifying the $x$-coordinates for which measured values are provided.\n",
    "3. `ydata`: A `numpy.array` specifying the measured values to be fitted using `f`.\n",
    "4. Keyword argument `p0`: An _initial guess_ for the value of $\\hat{\\mathbf{\\Theta}}$ supplied as a _Python_ `tuple` type. A value of `(0.5,1.5,1.0,0.7)` is used to demonstrate that the `curve_fit` function does indeed vary the elements of $\\hat{\\mathbf{\\Theta}}$ such that they converge the towards the \"_true_\" values of the elements of $\\mathbf{\\Theta}$.\n",
    "\n",
    "The `scipy.optimize.curve_fit(...)` function returns a sequence containing two elements. The first element of the returned sequence is a one-dimensional `numpy.array` containing the best-fitting values of the $\\hat{\\mathbf{\\Theta}}$ that were determined by `scipy.optimize.curve_fit(...)`. The second element is a two-dimensional `numpy.array` containing the **covariance matrix** that can be used to determine any correlation between the model parameters as well as the uncertainty that is associated with an optimized parameter value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Define a Python tuple containing four initial guesses for the\n",
    "values of the parameters of \n",
    "'''\n",
    "guessedPars = (0.5,1.5,1.0,0.7)\n",
    "\n",
    "'''\n",
    "Invoke the scipy.optimize.curve_fit(...) function to determine the set of \n",
    "model parameters that best replicate the observed data.\n",
    "'''\n",
    "optimalPars, covMat = scipy.optimize.curve_fit(modelFunction, xValues, yValuesDirty, p0=guessedPars)\n",
    "\n",
    "print \"Optimized parameter values:\\n\",optimalPars \n",
    "print \"\\nDifference from true parameter values:\\n\",optimalPars - trueParameters\n",
    "print \"\\nDifference from guessed parameter values:\\n\",optimalPars - guessedPars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 6: Plot the Results of the Fit to a Binned Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Adjust the size of the current figure\n",
    "pyplot.figure(figsize=(10, 8))\n",
    "\n",
    "''' Plot three curves corresponding to:\n",
    "1. The fitted function samples.\n",
    "2. The noise-free function samples.\n",
    "3. The noisy function samples.\n",
    "'''\n",
    "yValuesFitted = modelFunction(xValues, optimalPars[0], optimalPars[1], optimalPars[2], optimalPars[3])\n",
    "plottedCurves = pyplot.plot(xValues, yValuesFitted, xValues, yValuesClean, xValues, yValuesDirty)\n",
    "\n",
    "'''\n",
    "Add a plot legend and adjust the y-limits of the current axes\n",
    "to accommodate its extent.\n",
    "'''\n",
    "pyplot.legend((plottedCurves[0], plottedCurves[1], plottedCurves[2]), (\"Fitted\",\"Clean\",\"Noisy\"), loc='lower center', ncol=3)\n",
    "newLimits = pyplot.ylim(-1, 1.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling Special File Formats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Often retrictions imposed by laboratory facilities, a legacy of previous experience, or the requirements of an experimental technique mandate the use of a software package that uses a proprietary file format. \n",
    "\n",
    "Occasionally it is not convenient to analyse the experimental data using the tool that was used to obtain them. Fortunately, the `scipy` package includes the `io` subpackage, which provides functions that enable data to be extracted from and (in many cases) written to files with formats that are commonly encountered in the course of scientific research. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MATLAB Files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `scipy.io` subpackage provides support for **reading** and **writing** files that are compatible with the [_MATLAB_](http://www.mathworks.com/products/matlab/) data analysis utility. \n",
    "\n",
    "#### Step 1: Importing the `scipy.io` Subpackage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy import io"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2: Writing to a File Using the _MATLAB_ File Format\n",
    "\n",
    "Data can be written to [_MATLAB_ compatible files](http://www.mathworks.com/help/pdf_doc/matlab/matfile_format.pdf) using the `scipy.io.savemat(...)` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "help(scipy.io.savemat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Python_ `dict` types are used to aggregate the data that are to be saved in a _MATLAB_-compatible format. The `dict` keys **must** be _Python_ `str`-type variables or literals, but numerous __Python__ and `numpy` types are permitted for the associated `dict` values.\n",
    "\n",
    "The subsequent material in this section demonstrates that\n",
    "\n",
    "* If the value associated with a particular `dict` key is a `numpy.array`, then that value will be saved as a **_MATLAB_ Array Data** type.\n",
    "* If the value associated with a particular `dict` key is **itself** a `dict` with `str`-type keys, then that value will be saved as a **_MATLAB_ struct** type.\n",
    "\n",
    "For details about other types that can be written using the `scipy.io.savemat(...)` function, consult the [online documentation](https://docs.scipy.org/doc/scipy-0.15.1/reference/tutorial/io.html#matlab-files)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Aggregate the arrays containing samples of the clean, noisy, and \n",
    "fitted functions into a dict with str-type keys. This results in \n",
    "a MATLAB struct-type being saved.\n",
    "'''\n",
    "yValuesAll = {\"yValuesClean\" : yValuesClean, \"yValuesDirty\" : yValuesDirty, \"yValuesFitted\" : yValuesFitted }\n",
    "'''\n",
    "Invoke the scipy.io.savemat(...) function to write the specified\n",
    "data to a file called \"testMatlabFile.mat\" (the suffix is appended\n",
    "automatically). Providing the sequence of sampled x-values as a\n",
    "numpy.array with cause the to be stored as a MATLAB Array-Data-type\n",
    "file entry.\n",
    "'''\n",
    "scipy.io.savemat('testMatlabFile', {\"xValues\" : xValues, \"yValues\" : yValuesAll})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3: Examining the contents of a _MATLAB_-compatible File\n",
    "In principle, _MATLAB_ compatible files may contain large quantities of compressed data. Reading these data into computer memory may not be desirable (or even fruitful if the format is particularly obscure). Instead, it may be sufficient to examine **reduced metadata** that describes the types and properties of the data a file contains.\n",
    "\n",
    "The `scipy.io.whosmat(...)` function provides this functionality. Invoking the function returns a list of 3-element tuples. Each tuple corresponds to a particular data entry in the file and reports the _entry key_, the _dimensionality and extent_ of the corresponding data, and its _associated MATLAB type_. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Inspect the metadata that describe the contents of a \n",
    "MATLAB-compatible file.\n",
    "'''\n",
    "matlabFileMetadata = scipy.io.whosmat('testMatlabFile')\n",
    "print matlabFileMetadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 4: Extracting Data from _MATLAB_-compatible Files\n",
    "The `scipy.io.loadmat(...)` function is provided to open a _MATLAB_-compatible file and load its contents into computer memory.\n",
    "\n",
    "The function has a single **required** argument which **must** be a `str`-type value encoding the path of the file to be opened.\n",
    "\n",
    "Several optional keyword arguments are also available to control how the extracted data are preprocessed and formattted before they are returned by `scipy.io.loadmat(...)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "help(scipy.io.loadmat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Open a MATLAB-compatible file and extract the data it contains.\n",
    "matlabFileContents = scipy.io.loadmat('testMatlabFile')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 5: Parsing and Analysing the Extracted Data\n",
    "\n",
    "In general, opening a MATLAB file using the `scipy.io.loadmat(...)` function returns a _Python_ `dict`-type with `str`-type keys and arbitrarily typed values. Consider the concrete example of the `dict` returned when `scipy.io.loadmat(...)` is invoked to load data from the \"`testMatlabFile.mat`\" file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print \"matlabFileContents:\\n\", matlabFileContents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The first element** of the \"`matlabFileContents`\" `dict` that is returned has the `str`-type key \"`xValues`\", which maps to a **two-dimensional** `numpy.array`. The **innermost** (zeroth) dimension contains 50 `float`-type values corresponding to the 50 $x$ values at which the $y$ values were sampled. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print \"Dimensionality of xValues:\\n\", numpy.shape(matlabFileContents['xValues'])\n",
    "\n",
    "print \"\\nSize of xValues[0]:\\n\", len(matlabFileContents['xValues'][0])\n",
    "\n",
    "print \"\\nContent of xValues[0]:\\n\", matlabFileContents['xValues'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **third element** has the `str`-type key \"`yValues`\", which maps to a **deeply nested structure** of `numpy.array`s that can be appropriately indexed to extract 50-element `numpy.arrays` containing samples of the _Clean_, _Dirty_ and _Fitted_ arrays.\n",
    "\n",
    "The subsequent cells examine the properties of the data structure at increasing levels of nesting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print \"Dimensionality of yValues:\\n\", numpy.shape(matlabFileContents['yValues'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print \"Type of yValues[0]:\\n\",type(matlabFileContents['yValues'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print \"Dimensionality of yValues[0]:\\n\", numpy.shape(matlabFileContents['yValues'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print \"Content of yValues[0]:\\n\", matlabFileContents['yValues'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print \"Type of yValues[0][0]:\\n\", type(matlabFileContents['yValues'][0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** The `numpy.void` type is designed to be a **flexible type**. In this case it behaves as a sequence with 3 elements. Consult this [online documentation](http://docs.scipy.org/doc/numpy/reference/arrays.dtypes.html) about `numpy` data types for more information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print \"Size of yValues[0][0]:\\n\", len(matlabFileContents['yValues'][0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print \"\\nContent of yValues[0][0]:\\n\", matlabFileContents['yValues'][0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print \"\\nType of yValues[0][0][0]:\\n\", type(matlabFileContents['yValues'][0][0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print \"\\nSize of yValues[0][0][0]:\\n\", len(matlabFileContents['yValues'][0][0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print \"\\nContent of yValues[0][0][0]:\\n\", matlabFileContents['yValues'][0][0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print \"\\nType of yValues[0][0][0][0]:\\n\", type(matlabFileContents['yValues'][0][0][0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print \"\\nSize of yValues[0][0][0][0]:\\n\", len(matlabFileContents['yValues'][0][0][0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print \"\\nContent of yValues[0][0][0][0]:\\n\", matlabFileContents['yValues'][0][0][0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 6: Plotting the extracted data\n",
    "\n",
    "Now that the locations of the various data have been established it can be plotted in order to ensure that it was faithfully stored and recovered. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Adjust the size of the current figure\n",
    "pyplot.figure(figsize=(10, 8))\n",
    "\n",
    "'''\n",
    "The data are stored in nested sequences. Accordingly, a loop can be used to\n",
    "the plot multiple series using multiple invocations of pyplot.plot(...).\n",
    "\n",
    "Begin by defining an empty list to hold the matplotlib.lines.Line2D instances\n",
    "that are returned by pyplot.plot(...).\n",
    "'''\n",
    "plottedCurves = []\n",
    "\n",
    "'''\n",
    "Recall that the individual y-value series are stored in doubly nested \n",
    "numpy.arrays populating the 3 elements of a flexible numpy.void type \n",
    "that was loaded into memory at:\n",
    "\n",
    "matlabFileContents['yValues'][0][0].\n",
    "\n",
    "Use a for-loop to iterate over the elements of the numpy.void type.\n",
    "The loop variable 'yData' is a single element numpy array.\n",
    "'''\n",
    "for yData in matlabFileContents['yValues'][0][0] :\n",
    "    '''\n",
    "    All the stored y-value series correspond to a single x-value series\n",
    "    that was loaded into memory at matlabFileContents['xValues'][0].\n",
    "    \n",
    "    The y-value series data are numpy.arrays occupying the zeroth element\n",
    "    of the 'yData' variable\n",
    "    \n",
    "    Invoke the pyplot.plot(...) function to render a the current y-value\n",
    "    series versus the correspondin x coordinates.\n",
    "    '''\n",
    "    plottedCurve = pyplot.plot(matlabFileContents['xValues'][0], yData[0])\n",
    "    \n",
    "    '''\n",
    "    Append the matplotlib.lines.Line2D instance that was returned by the\n",
    "    invocation of pyplot.plot(...) to the list of all plotted curves.\n",
    "    '''\n",
    "    plottedCurves.append(plottedCurve[0])\n",
    "    \n",
    "# Generate a legend to annotate each of the plotted curves\n",
    "pyplot.legend((plottedCurves[0], plottedCurves[1], plottedCurves[2]), (\"Noisy\",\"Clean\",\"Fitted\"), loc='lower center', ncol=3)\n",
    "\n",
    "# Adjust the limits of the current y-axis to accommodate the plot legend\n",
    "newLimits = pyplot.ylim(-1, 1.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IDL Files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `scipy` package also provides the `scipy.io.readsav(...)` function that enables files that are produced using the [_Interactive Data Language_](http://www.exelisvis.com/ProductsServices/IDL.aspx) (IDL). Unfortunately, **writing** of IDL-format files is **not supported**.\n",
    "\n",
    "As usual, more information about the `scipy.io.readsav(...)` function is available [online](https://docs.scipy.org/doc/scipy-0.15.1/reference/generated/scipy.io.readsav.html#scipy.io.readsav) or inline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "help (scipy.io.readsav)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
